@inproceedings{Abbas_P_S11,
  title = {Improved {{Algorithms}} for {{Linear Stochastic Bandits}}},
  author = {{Abbasi-Yadkori}, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  year = {2011},
  pages = {2312--2320}
}

@misc{Agarw_J_Z22,
  title = {{{VO}}\${{Q}}\${{L}}: {{Towards Optimal Regret}} in {{Model-free RL}} with {{Nonlinear Function Approximation}}},
  shorttitle = {{{VO}}\${{Q}}\${{L}}},
  author = {Agarwal, Alekh and Jin, Yujia and Zhang, Tong},
  year = {2022},
  number = {arXiv:2212.06069}
}

@misc{Agarw_K_L_M20,
  title = {On the {{Theory}} of {{Policy Gradient Methods}}: {{Optimality}}, {{Approximation}}, and {{Distribution Shift}}},
  shorttitle = {On the {{Theory}} of {{Policy Gradient Methods}}},
  author = {Agarwal, Alekh and Kakade, Sham M. and Lee, Jason D. and Mahajan, Gaurav},
  year = {2020},
  number = {arXiv:1908.00261}
}

@misc{Agarw_Z22,
  title = {Model-Based {{RL}} with {{Optimistic Posterior Sampling}}: {{Structural Conditions}} and {{Sample Complexity}}},
  shorttitle = {Model-Based {{RL}} with {{Optimistic Posterior Sampling}}},
  author = {Agarwal, Alekh and Zhang, Tong},
  year = {2022},
  number = {arXiv:2206.07659}
}

@misc{Agarw_Z22a,
  title = {Non-{{Linear Reinforcement Learning}} in {{Large Action Spaces}}: {{Structural Conditions}} and {{Sample-efficiency}} of {{Posterior Sampling}}},
  shorttitle = {Non-{{Linear Reinforcement Learning}} in {{Large Action Spaces}}},
  author = {Agarwal, Alekh and Zhang, Tong},
  year = {2022},
  number = {arXiv:2203.08248}
}

@misc{Agarw_Z22b,
  title = {Non-{{Linear Reinforcement Learning}} in {{Large Action Spaces}}: {{Structural Conditions}} and {{Sample-efficiency}} of {{Posterior Sampling}}},
  shorttitle = {Non-{{Linear Reinforcement Learning}} in {{Large Action Spaces}}},
  author = {Agarwal, Alekh and Zhang, Tong},
  year = {2022},
  number = {arXiv:2203.08248}
}

@article{Audib_M_S09,
  title = {Exploration\textendash Exploitation Tradeoff Using Variance Estimates in Multi-Armed Bandits},
  author = {Audibert, Jean-Yves and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  year = {2009},
  journal = {Theoretical Computer Science},
  volume = {410},
  number = {19},
  pages = {1876--1902},
  doi = {10.1016/j.tcs.2009.01.016}
}

@misc{Azar_O_M17,
  title = {Minimax {{Regret Bounds}} for {{Reinforcement Learning}}},
  author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  year = {2017},
  number = {arXiv:1703.05449}
}

@misc{Bacon_H_P16,
  title = {The {{Option-Critic Architecture}}},
  author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  year = {2016},
  number = {arXiv:1609.05140}
}

@misc{Bhand_R_S18,
  title = {A {{Finite Time Analysis}} of {{Temporal Difference Learning With Linear Function Approximation}}},
  author = {Bhandari, Jalaj and Russo, Daniel and Singal, Raghav},
  year = {2018},
  number = {arXiv:1806.02450}
}

@misc{Bhand_R22,
  title = {Global {{Optimality Guarantees For Policy Gradient Methods}}},
  author = {Bhandari, Jalaj and Russo, Daniel},
  year = {2022},
  number = {arXiv:1906.01786}
}

@article{Borka_M00,
  title = {The {{O}}.{{D}}.{{E}}. {{Method}} for {{Convergence}} of {{Stochastic Approximation}} and {{Reinforcement Learning}}},
  author = {Borkar, V. S. and Meyn, S. P.},
  year = {2000},
  journal = {SIAM Journal on Control and Optimization},
  volume = {38},
  number = {2},
  pages = {447--469},
  doi = {10.1137/S0363012997331639}
}

@misc{Bubec_C12,
  title = {Regret {{Analysis}} of {{Stochastic}} and {{Nonstochastic Multi-armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien and {Cesa-Bianchi}, Nicol{\`o}},
  year = {2012},
  number = {arXiv:1204.5721}
}

@misc{Bubec_S22,
  title = {First-{{Order Bayesian Regret Analysis}} of {{Thompson Sampling}}},
  author = {Bubeck, S{\'e}bastien and Sellke, Mark},
  year = {2022},
  number = {arXiv:1902.00681}
}

@misc{Chen_M_B22,
  title = {Unified {{Algorithms}} for {{RL}} with {{Decision-Estimation Coefficients}}: {{No-Regret}}, {{PAC}}, and {{Reward-Free Learning}}},
  shorttitle = {Unified {{Algorithms}} for {{RL}} with {{Decision-Estimation Coefficients}}},
  author = {Chen, Fan and Mei, Song and Bai, Yu},
  year = {2022},
  number = {arXiv:2209.11745}
}

@misc{Cheng_X_J_A22,
  title = {Adversarially {{Trained Actor Critic}} for {{Offline Reinforcement Learning}}},
  author = {Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  year = {2022},
  number = {arXiv:2202.02446}
}

@inproceedings{DBLP:conf/icml/X19,
  title = {{{POLITEX}}: {{Regret}} Bounds for Policy Iteration Using Expert Prediction},
  year = {2019},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {3692--3702},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Tue, 08 Oct 2019 16:41:22 +0200}
}

@inproceedings{DBLP:conf/nips/DannMMZ21,
  title = {Beyond Value-Function Gaps: {{Improved}} Instance-Dependent Regret Bounds for Episodic Reinforcement Learning},
  author = {Dann, Christoph and Marinov, Teodor Vanislavov and Mohri, Mehryar and Zimmert, Julian},
  year = {2021},
  pages = {1--12},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Tue, 03 May 2022 16:20:46 +0200}
}

@article{deFa_R03,
  title = {The Linear Programming Approach to Approximate Dynamic Programming},
  author = {{de Farias}, Daniela Pucci and Roy, Benjamin Van},
  year = {2003},
  journal = {Operations Research},
  volume = {51},
  number = {6},
  pages = {850--865},
  doi = {10.1287/OPRE.51.6.850.24925},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Tue, 31 Mar 2020 18:16:33 +0200}
}

@misc{Du_K_L_L_M_S_W21,
  title = {Bilinear {{Classes}}: {{A Structural Framework}} for {{Provable Generalization}} in {{RL}}},
  shorttitle = {Bilinear {{Classes}}},
  author = {Du, Simon S. and Kakade, Sham M. and Lee, Jason D. and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  year = {2021},
  number = {arXiv:2103.10897}
}

@misc{Du_K_W_Y20,
  title = {Is a {{Good Representation Sufficient}} for {{Sample Efficient Reinforcement Learning}}?},
  author = {Du, Simon S. and Kakade, Sham M. and Wang, Ruosong and Yang, Lin F.},
  year = {2020},
  number = {arXiv:1910.03016}
}

@misc{Foste_G_Q_R_S22,
  title = {A {{Note}} on {{Model-Free Reinforcement Learning}} with the {{Decision-Estimation Coefficient}}},
  author = {Foster, Dylan J. and Golowich, Noah and Qian, Jian and Rakhlin, Alexander and Sekhari, Ayush},
  year = {2022},
  number = {arXiv:2211.14250}
}

@misc{Foste_K_Q_R22,
  title = {The {{Statistical Complexity}} of {{Interactive Decision Making}}},
  author = {Foster, Dylan J. and Kakade, Sham M. and Qian, Jian and Rakhlin, Alexander},
  year = {2022},
  number = {arXiv:2112.13487}
}

@misc{Foste_K21,
  title = {Efficient {{First-Order Contextual Bandits}}: {{Prediction}}, {{Allocation}}, and {{Triangular Discrimination}}},
  shorttitle = {Efficient {{First-Order Contextual Bandits}}},
  author = {Foster, Dylan J. and Krishnamurthy, Akshay},
  year = {2021},
  number = {arXiv:2107.02237}
}

@misc{Foste_R_S_S22,
  title = {On the {{Complexity}} of {{Adversarial Decision Making}}},
  author = {Foster, Dylan J. and Rakhlin, Alexander and Sekhari, Ayush and Sridharan, Karthik},
  year = {2022},
  number = {arXiv:2206.13063}
}

@misc{Foste_R20,
  title = {Beyond {{UCB}}: {{Optimal}} and {{Efficient Contextual Bandits}} with {{Regression Oracles}}},
  shorttitle = {Beyond {{UCB}}},
  author = {Foster, Dylan J. and Rakhlin, Alexander},
  year = {2020},
  number = {arXiv:2002.04926}
}

@misc{Fruit_L17,
  title = {Exploration--{{Exploitation}} in {{MDPs}} with {{Options}}},
  author = {Fruit, Ronan and Lazaric, Alessandro},
  year = {2017},
  number = {arXiv:1703.08667}
}

@article{Gouve_G_O_S23,
  title = {Thompson {{Sampling Regret Bounds}} for {{Contextual Bandits}} with Sub-{{Gaussian}} Rewards},
  author = {Gouverneur, Amaury and G{\'a}lvez, Borja Rodr{\'i}guez and Oechtering, Tobias J. and Skoglund, Mikael},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2304.13593},
  doi = {10.48550/arXiv.2304.13593}
}

@misc{Hao_L_Q22,
  title = {Contextual {{Information-Directed Sampling}}},
  author = {Hao, Botao and Lattimore, Tor and Qin, Chao},
  year = {2022},
  number = {arXiv:2205.10895}
}

@misc{Hao_L22,
  title = {Regret {{Bounds}} for {{Information-Directed Reinforcement Learning}}},
  author = {Hao, Botao and Lattimore, Tor},
  year = {2022},
  number = {arXiv:2206.04640}
}

@article{Hsu_K_Z14,
  title = {Random {{Design Analysis}} of {{Ridge Regression}}},
  author = {Hsu, Daniel J. and Kakade, Sham M. and Zhang, Tong},
  year = {2014},
  journal = {Found. Comput. Math.},
  volume = {14},
  number = {3},
  pages = {569--600},
  doi = {10.1007/s10208-014-9192-1}
}

@misc{Hu_J_T22,
  title = {Actor-Critic Is Implicitly Biased towards High Entropy Optimal Policies},
  author = {Hu, Yuzheng and Ji, Ziwei and Telgarsky, Matus},
  year = {2022},
  number = {arXiv:2110.11280}
}

@article{Huang_C_J23,
  title = {Reinforcement {{Learning}} in {{Low-Rank MDPs}} with {{Density Features}}},
  author = {Huang, Audrey and Chen, Jinglin and Jiang, Nan},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2302.02252},
  doi = {10.48550/arXiv.2302.02252}
}

@inproceedings{Jin_A_B_J18,
  title = {Is {{Q-Learning Provably Efficient}}?},
  author = {Jin, Chi and {Allen-Zhu}, Zeyuan and Bubeck, S{\'e}bastien and Jordan, Michael I.},
  year = {2018},
  pages = {4868--4878}
}

@inproceedings{Jin_J_L_S_Y20,
  title = {Learning {{Adversarial Markov Decision Processes}} with {{Bandit Feedback}} and {{Unknown Transition}}},
  author = {Jin, Chi and Jin, Tiancheng and Luo, Haipeng and Sra, Suvrit and Yu, Tiancheng},
  year = {2020},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {119},
  pages = {4860--4869}
}

@inproceedings{Jin_L_M21,
  title = {Bellman {{Eluder Dimension}}: {{New Rich Classes}} of {{RL Problems}}, and {{Sample-Efficient Algorithms}}},
  author = {Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  year = {2021},
  pages = {13406--13418}
}

@misc{Jin_S20,
  title = {Efficiently {{Solving MDPs}} with {{Stochastic Mirror Descent}}},
  author = {Jin, Yujia and Sidford, Aaron},
  year = {2020},
  number = {arXiv:2008.12776}
}

@misc{Jin_Y_W_J19,
  title = {Provably {{Efficient Reinforcement Learning}} with {{Linear Function Approximation}}},
  author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I.},
  year = {2019},
  number = {arXiv:1907.05388}
}

@misc{Judit_N23,
  title = {Large {{Deviations}} of {{Vector-valued Martingales}} in 2-{{Smooth Normed Spaces}}},
  author = {Juditsky, Anatoli and Nemirovski, Arkadii S.},
  year = {2023},
  number = {arXiv:0809.0813}
}

@misc{Kirsc_K18,
  title = {Information {{Directed Sampling}} and {{Bandits}} with {{Heteroscedastic Noise}}},
  author = {Kirschner, Johannes and Krause, Andreas},
  year = {2018},
  number = {arXiv:1801.09667}
}

@misc{Kong_Z_W_L23,
  title = {Improved {{Regret Bounds}} for {{Linear Adversarial MDPs}} via {{Linear Optimization}}},
  author = {Kong, Fang and Zhang, Xiangcheng and Wang, Baoxiang and Li, Shuai},
  year = {2023},
  number = {arXiv:2302.06834}
}

@misc{Lamb_I_E_D_M_F_M_C_K_L22,
  title = {Guaranteed {{Discovery}} of {{Control-Endogenous Latent States}} with {{Multi-Step Inverse Models}}},
  author = {Lamb, Alex and Islam, Riashat and Efroni, Yonathan and Didolkar, Aniket and Misra, Dipendra and Foster, Dylan and Molu, Lekan and Chari, Rajan and Krishnamurthy, Akshay and Langford, John},
  year = {2022},
  number = {arXiv:2207.08229}
}

@inproceedings{Latti_G21,
  title = {Mirror {{Descent}} and the {{Information Ratio}}},
  author = {Lattimore, Tor and Gy{\"o}rgy, Andr{\'a}s},
  year = {2021},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {134},
  pages = {2965--2992}
}

@inproceedings{Latti_S19,
  title = {An {{Information-Theoretic Approach}} to {{Minimax Regret}} in {{Partial Monitoring}}},
  author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year = {2019},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {99},
  pages = {2111--2139}
}

@misc{Lu_V_D_I_O_W22,
  title = {Reinforcement {{Learning}}, {{Bit}} by {{Bit}}},
  author = {Lu, Xiuyuan and Van Roy, Benjamin and Dwaracherla, Vikranth and Ibrahimi, Morteza and Osband, Ian and Wen, Zheng},
  year = {2022},
  number = {arXiv:2103.04047}
}

@misc{Maure_P09,
  title = {Empirical {{Bernstein Bounds}} and {{Sample Variance Penalization}}},
  author = {Maurer, Andreas and Pontil, Massimiliano},
  year = {2009},
  number = {arXiv:0907.3740}
}

@misc{Mhamm_F_R23,
  title = {Representation {{Learning}} with {{Multi-Step Inverse Kinematics}}: {{An Efficient}} and {{Optimal Approach}} to {{Rich-Observation RL}}},
  shorttitle = {Representation {{Learning}} with {{Multi-Step Inverse Kinematics}}},
  author = {Mhammedi, Zakaria and Foster, Dylan J. and Rakhlin, Alexander},
  year = {2023},
  number = {arXiv:2304.05889}
}

@misc{Neu_O_P_S22,
  title = {Lifting the {{Information Ratio}}: {{An Information-Theoretic Analysis}} of {{Thompson Sampling}} for {{Contextual Bandits}}},
  shorttitle = {Lifting the {{Information Ratio}}},
  author = {Neu, Gergely and Olkhovskaya, Julia and Papini, Matteo and Schwartz, Ludovic},
  year = {2022},
  number = {arXiv:2205.13924}
}

@misc{Neu_P20,
  title = {A {{Unifying View}} of {{Optimism}} in {{Episodic Reinforcement Learning}}},
  author = {Neu, Gergely and {Pike-Burke}, Ciara},
  year = {2020},
  number = {arXiv:2007.01891}
}

@misc{Nguye_Y_G_V_A23,
  title = {On {{Instance-Dependent Bounds}} for {{Offline Reinforcement Learning}} with {{Linear Function Approximation}}},
  author = {{Nguyen-Tang}, Thanh and Yin, Ming and Gupta, Sunil and Venkatesh, Svetha and Arora, Raman},
  year = {2023},
  number = {arXiv:2211.13208}
}

@misc{ODon_23,
  title = {Efficient Exploration via Epistemic-Risk-Seeking Policy Optimization},
  author = {O'Donoghue, Brendan},
  year = {2023},
  number = {arXiv:2302.09339}
}

@inproceedings{Peter_V_S05,
  title = {Natural {{Actor-Critic}}},
  author = {Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
  year = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3720},
  pages = {280--291},
  doi = {10.1007/11564096_29}
}

@misc{Ren_Z_S_D21,
  title = {A {{Free Lunch}} from the {{Noise}}: {{Provable}} and {{Practical Exploration}} for {{Representation Learning}}},
  shorttitle = {A {{Free Lunch}} from the {{Noise}}},
  author = {Ren, Tongzheng and Zhang, Tianjun and Szepesv{\'a}ri, Csaba and Dai, Bo},
  year = {2021},
  number = {arXiv:2111.11485}
}

@misc{Russo_V15,
  title = {An {{Information-Theoretic Analysis}} of {{Thompson Sampling}}},
  author = {Russo, Daniel and Van Roy, Benjamin},
  year = {2015},
  number = {arXiv:1403.5341}
}

@misc{Russo_V17,
  title = {Learning to {{Optimize}} via {{Information-Directed Sampling}}},
  author = {Russo, Daniel and Van Roy, Benjamin},
  year = {2017},
  number = {arXiv:1403.5556}
}

@misc{Saha_K21,
  title = {Efficient and {{Optimal Algorithms}} for {{Contextual Dueling Bandits}} under {{Realizability}}},
  author = {Saha, Aadirupa and Krishnamurthy, Akshay},
  year = {2021},
  number = {arXiv:2111.12306}
}

@misc{Song_Z_S_B_K_S22,
  title = {Hybrid {{RL}}: {{Using Both Offline}} and {{Online Data Can Make RL Efficient}}},
  shorttitle = {Hybrid {{RL}}},
  author = {Song, Yuda and Zhou, Yifei and Sekhari, Ayush and Bagnell, J. Andrew and Krishnamurthy, Akshay and Sun, Wen},
  year = {2022},
  number = {arXiv:2210.06718}
}

@article{Srini_K_K_S12,
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}: {{No Regret}} and {{Experimental Design}}},
  shorttitle = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
  year = {2012},
  journal = {IEEE Transactions on Information Theory},
  volume = {58},
  number = {5},
  pages = {3250--3265},
  doi = {10.1109/TIT.2011.2182033}
}

@article{Sutto_M_S_M,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay}
}

@article{Tarbo_L_O,
  title = {Probabilistic {{Inference}} in {{Reinforcement Learning Done Right}}},
  author = {Tarbouriech, Jean and Lattimore, Tor and O'Donoghue, Brendan}
}

@misc{Tiapk_G22,
  title = {Primal-{{Dual Stochastic Mirror Descent}} for {{MDPs}}},
  author = {Tiapkin, Daniil and Gasnikov, Alexander},
  year = {2022},
  number = {arXiv:2103.00299}
}

@article{Tirin_M_K22,
  title = {Near {{Instance-Optimal PAC Reinforcement Learning}} for {{Deterministic MDPs}}},
  author = {Tirinzoni, Andrea and Marjani, Aymen Al and Kaufmann, Emilie},
  year = {2022},
  journal = {CoRR},
  volume = {abs/2203.09251},
  doi = {10.48550/arXiv.2203.09251}
}

@article{Tsits_V97,
  title = {An Analysis of Temporal-Difference Learning with Function Approximation},
  author = {Tsitsiklis, J.N. and Van Roy, B.},
  year = {1997},
  journal = {IEEE Transactions on Automatic Control},
  volume = {42},
  number = {5},
  pages = {674--690},
  doi = {10.1109/9.580874}
}

@article{Wagen_F23,
  title = {Instance-{{Optimality}} in {{Interactive Decision Making}}: {{Toward}} a {{Non-Asymptotic Theory}}},
  author = {Wagenmaker, Andrew and Foster, Dylan J.},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2304.12466},
  doi = {10.48550/arXiv.2304.12466}
}

@misc{Wan_N_S21,
  title = {Average-{{Reward Learning}} and {{Planning}} with {{Options}}},
  author = {Wan, Yi and Naik, Abhishek and Sutton, Richard S.},
  year = {2021},
  number = {arXiv:2110.13855}
}

@misc{Waudb_R22,
  title = {Estimating Means of Bounded Random Variables by Betting},
  author = {{Waudby-Smith}, Ian and Ramdas, Aaditya},
  year = {2022},
  number = {arXiv:2010.09686}
}

@misc{Wei_J_L_J21,
  title = {Learning {{Infinite-horizon Average-reward MDPs}} with {{Linear Function Approximation}}},
  author = {Wei, Chen-Yu and {Jafarnia-Jahromi}, Mehdi and Luo, Haipeng and Jain, Rahul},
  year = {2021},
  number = {arXiv:2007.11849}
}

@article{Wen_P_I_B,
  title = {On {{Efficiency}} in {{Hierarchical Reinforcement Learning}}},
  author = {Wen, Zheng and Precup, Doina and Ibrahimi, Morteza and Barreto, Andre}
}

@misc{Xiao_W_L_D_M_L_S_S21,
  title = {On the {{Optimality}} of {{Batch Policy Optimization Algorithms}}},
  author = {Xiao, Chenjun and Wu, Yifan and Lattimore, Tor and Dai, Bo and Mei, Jincheng and Li, Lihong and Szepesvari, Csaba and Schuurmans, Dale},
  year = {2021},
  number = {arXiv:2104.02293}
}

@misc{Xie_F_B_J_K22,
  title = {The {{Role}} of {{Coverage}} in {{Online Reinforcement Learning}}},
  author = {Xie, Tengyang and Foster, Dylan J. and Bai, Yu and Jiang, Nan and Kakade, Sham M.},
  year = {2022},
  number = {arXiv:2210.04157}
}

@inproceedings{Xu_Z_M_A_A22,
  title = {Langevin {{Monte Carlo}} for {{Contextual Bandits}}},
  author = {Xu, Pan and Zheng, Hongkai and Mazumdar, Eric V. and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  year = {2022},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {162},
  pages = {24830--24850}
}

@article{Zhang_06,
  title = {From \$\textbackslash epsilon\$-Entropy to {{KL-entropy}}: {{Analysis}} of Minimum Information Complexity Density Estimation},
  shorttitle = {From \$\textbackslash epsilon\$-Entropy to {{KL-entropy}}},
  author = {Zhang, Tong},
  year = {2006},
  journal = {The Annals of Statistics},
  volume = {34},
  number = {5},
  doi = {10.1214/009053606000000704}
}

@misc{Zhang_21,
  title = {Feel-{{Good Thompson Sampling}} for {{Contextual Bandits}} and {{Reinforcement Learning}}},
  author = {Zhang, Tong},
  year = {2021},
  number = {arXiv:2110.00871}
}

@inproceedings{Zimme_L19,
  title = {Connections {{Between Mirror Descent}}, {{Thompson Sampling}} and the {{Information Ratio}}},
  author = {Zimmert, Julian and Lattimore, Tor},
  year = {2019},
  pages = {11950--11959}
}

@misc{Zimme_L19a,
  title = {Connections {{Between Mirror Descent}}, {{Thompson Sampling}} and the {{Information Ratio}}},
  author = {Zimmert, Julian and Lattimore, Tor},
  year = {2019},
  number = {arXiv:1905.11817}
}
